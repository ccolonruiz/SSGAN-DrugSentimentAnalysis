{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "C1jcLQ8N-I-g",
    "outputId": "dabed6b0-8dd7-469b-c2e7-8ecd6752b475"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bert\n",
    "import pickle\n",
    "import keras as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, plot_model\n",
    "\n",
    "import Utils.data_utils as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = tf.config.experimental.list_physical_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokenizer(model_path):\n",
    "    \"\"\"This function aims to create a tokenizer specific to the format of BERT models. \n",
    "    BERT models contain different files, one of which is vocab.txt\n",
    "    - vocab: vocab.txt path\"\"\"\n",
    "    return bert.bert_tokenization.FullTokenizer(model_path+'/vocab.txt', do_lower_case=True)\n",
    "    \n",
    "def format_text(text, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    if len(tokens) > max_seq_len:\n",
    "        tokens = tokens[len(tokens)-max_seq_len:]\n",
    "        \n",
    "    input_sequence = [\"[CLS]\"]+tokens[:max_seq_len-2]+[\"[SEP]\"]\n",
    "    pad_len = max_seq_len-len(input_sequence)\n",
    "    return np.asarray(tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len).astype('int32')\n",
    "\n",
    "def save_processed(t_save_path, s_save_path, split=False):\n",
    "    \"\"\"\n",
    "    t Order: Xtrain, ytrain, Xdev, ydeb, Xtest, ytest\n",
    "    s Order: X_train, y_train, X_unl, y_unl\n",
    "    \"\"\"\n",
    "    with open(t_save_path,'wb') as f: pickle.dump([Xtrain, ytrain, Xdev, ydev, Xtest, ytest], f)\n",
    "    if split:\n",
    "        with open(s_save_path,'wb') as f: pickle.dump([X_train, y_train, X_unl, y_unl], f)\n",
    "\n",
    "def load_processed(t_save_path, s_save_path, split=False):\n",
    "    \"\"\"\n",
    "    t Order: Xtrain, ytrain, Xdev, ydeb, Xtest, ytest\n",
    "    s Order: X_train, y_train, X_unl, y_unl\n",
    "    \"\"\"\n",
    "    with open(t_save_path,'rb') as f: t = pickle.load(f)\n",
    "    if split:\n",
    "        with open(s_save_path,'rb') as f: s = pickle.load(f)\n",
    "        return t, s\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13nkL0zO_rTp"
   },
   "outputs": [],
   "source": [
    "max_seq_len = 250\n",
    "path_bert_model = './BERT-models/BERT-Mini'\n",
    "# path_bert_model = './BERT-models/BERT-Mini'\n",
    "\n",
    "path_train = 'dataset/Raw/3c_Train.csv'\n",
    "path_test = 'dataset/Raw/3c_Test.csv'\n",
    "\n",
    "dataset = data.Dataset()\n",
    "dataset.load_csv(path_train, path_test, label_name='blabel', separator='\\t')\n",
    "dataset.make_dev_split(dev_split=0.15)\n",
    "fig = dataset.classes_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, s = load_processed(\"../GAN_BERT/dataset/splitedData/3c_BERT_processed_Data/5percent/Total\", \\\n",
    "               \"../GAN_BERT/dataset/splitedData/3c_BERT_processed_Data/5percent/Split\", split=True)\n",
    "Xtrain, ytrain, Xdev, ydev, Xtest, ytest = t\n",
    "X_train, y_train, X_unl, y_unl = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (Only if processed data is not available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_train().review = dataset.get_train().review.replace('\"','', regex=True)\n",
    "dataset.get_dev().review = dataset.get_dev().review.replace('\"','', regex=True)\n",
    "dataset.get_test().review = dataset.get_test().review.replace('\"','', regex=True)\n",
    "\n",
    "tokenizer = createTokenizer(path_bert_model)\n",
    "\n",
    "Xtrain = np.asarray([format_text(text, tokenizer) for text in dataset.get_train().review])\n",
    "Xtest = np.asarray([format_text(text, tokenizer) for text in dataset.get_test().review])\n",
    "Xdev = np.asarray([format_text(text, tokenizer) for text in dataset.get_dev().review])\n",
    "\n",
    "ytrain = to_categorical(dataset.get_train_y()-dataset.get_train_y().min())\n",
    "ytest = to_categorical(dataset.get_test_y()-dataset.get_test_y().min())\n",
    "ydev = to_categorical(dataset.get_dev_y()-dataset.get_dev_y().min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unl_ratio = 0.80 # 1 - unl_ratio = % train\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=unl_ratio, random_state=0)\n",
    "for train_index, test_index in sss.split(Xtrain, ytrain):\n",
    "    X_train, X_unl = Xtrain[train_index], Xtrain[test_index]\n",
    "    y_train, y_unl = ytrain[train_index], ytrain[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_processed(\"../GAN_BERT/dataset/splitedData/3c_BERT_processed_Data/20percent/Total\", \"../GAN_BERT/dataset/splitedData/3c_BERT_processed_Data/20percent/Split\", split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, s = load_processed(\"dataset/splitedData/3c_BERT_processed_Data/1percent/Total\", \\\n",
    "               \"dataset/splitedData/3c_BERT_processed_Data/1percent/Split\", split=True)\n",
    "Xtrain, ytrain, Xdev, ydev, Xtest, ytest = t\n",
    "X_train, y_train, X_unl, y_unl = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VGKcnmx_4pg"
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = './model/5percent_3c_CLS_aBERT_Best_EMRS_model_f1_Mini'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_f1_score',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_input_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32')\n",
    "\n",
    "bert_params = bert.params_from_pretrained_ckpt(path_bert_model)\n",
    "bert_params.adapter_size = 4\n",
    "bert_params.adapter_init_scale = 1e-5\n",
    "l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "embedded_sequences = l_bert(l_input_ids) # output: [batch_size, max_seq_len, hidden_size]\n",
    "drop_out_1 = keras.layers.Dropout(0.3, name='drop_out_1')(embedded_sequences)\n",
    "\n",
    "biLSTM_1 = keras.layers.Bidirectional(keras.layers.LSTM(250))(drop_out_1)\n",
    "\n",
    "drop_out_2 = keras.layers.Dropout(0.3, name='drop_out_2')(biLSTM_1)\n",
    "mp_dense = keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(), name='mp_dense')(drop_out_2)\n",
    "preds = keras.layers.Dense(3, activation='softmax', name='preds')(mp_dense)\n",
    "\n",
    "model = keras.Model(inputs=l_input_ids, outputs=preds)\n",
    "model.build(input_shape=(None,max_seq_len))\n",
    "\n",
    "l_bert.apply_adapter_freeze()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy',tfa.metrics.F1Score(num_classes=3, average='macro')])\n",
    "\n",
    "#callbacks_list = [metrics]\n",
    "# l_bert.apply_adapter_freeze()\n",
    "bert_ckpt_file = os.path.join(path_bert_model, \"bert_model.ckpt\")\n",
    "bert.load_stock_weights(l_bert, bert_ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "MVXc34ZYAKwY",
    "outputId": "8f4ddfc9-84a5-445c-f4e9-9cd09c999106",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "      #class_weight=class_weight_dict,\n",
    "      shuffle=True,\n",
    "      batch_size=32,\n",
    "      epochs=200,\n",
    "      #callbacks=[model_checkpoint_callback],\n",
    "      validation_data=(Xdev, ydev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rAEKWPm9AN6B"
   },
   "outputs": [],
   "source": [
    "y_test = ytest.argmax(axis=1)\n",
    "result = model.predict(Xtest).argmax(axis=-1)\n",
    "print(classification_report(y_test, result, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load last saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model = keras.models.load_model(checkpoint_filepath, custom_objects={'F1Score': tfa.metrics.F1Score(num_classes=3, average='macro')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "probas = last_model.predict(Xtest)\n",
    "result = probas.argmax(axis=-1)\n",
    "print(classification_report(y_test, result, digits=4))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fit_BERT_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
