{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import bert\n",
    "#import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.distribute import distribution_strategy_context as distribute_ctx\n",
    "from tensorflow.python.ops import clip_ops\n",
    "\n",
    "import Utils.data_utils as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = tf.config.experimental.list_physical_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokenizer(model_path):\n",
    "    \"\"\"This function aims to create a tokenizer specific to the format of BERT models. \n",
    "    BERT models contain different files, one of which is vocab.txt\n",
    "    - vocab: vocab.txt path\"\"\"\n",
    "    return bert.bert_tokenization.FullTokenizer(model_path+'/vocab.txt', do_lower_case=True)\n",
    "    \n",
    "def format_text(text, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    if len(tokens) > max_seq_len:\n",
    "        tokens = tokens[len(tokens)-max_seq_len:]\n",
    "        \n",
    "    input_sequence = [\"[CLS]\"]+tokens[:max_seq_len-2]+[\"[SEP]\"]\n",
    "    pad_len = max_seq_len-len(input_sequence)\n",
    "    return np.asarray(tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len).astype('int32')\n",
    "\n",
    "def StratifiedBatches(s, batch_size):\n",
    "    X_train, y_train = s\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    X_train, y_train = X_train[idx], y_train[idx]\n",
    "    \n",
    "    total_index = [i for i in range(0, len(X_train))]\n",
    "    #lab_index = np.where(np.any(y_train, axis=-1))[-1]\n",
    "    #unl_index = np.delete(total_index, lab_index, axis=0)\n",
    "    lab_index = np.where(y_train[:,0]==0)[-1]\n",
    "    unl_index = np.where(y_train[:,0]==1)[-1]\n",
    "\n",
    "    n_splits = int(np.ceil(X_train.shape[0]/batch_size))\n",
    "    \n",
    "    splits_index = [np.array_split(split, n_splits) for split in [lab_index, unl_index]]\n",
    "    \n",
    "    x = [0]*n_splits\n",
    "    y = [0]*n_splits\n",
    "    for i in range(0, n_splits):\n",
    "        x[i] = [X_train[x] for x in np.concatenate((splits_index[0][i], splits_index[1][i]), axis=0)]\n",
    "        y[i] = [y_train[x] for x in np.concatenate((splits_index[0][i], splits_index[1][i]), axis=0)]\n",
    "        idx = np.random.permutation(len(x[i]))\n",
    "        x[i], y[i] = np.array(x[i])[idx], np.array(y[i])[idx]\n",
    "        \n",
    "    return x, y\n",
    "\n",
    "def get_label_mask(y_true):\n",
    "    return 0.5 > y_true[:,0]\n",
    "\n",
    "def metric_mask_filter(y_true, y_pred):\n",
    "    mask = get_label_mask(y_true)\n",
    "    y_true = tf.boolean_mask(y_true, mask)\n",
    "    y_pred = tf.boolean_mask(y_pred, mask)\n",
    "    return y_true[:,1:], y_pred[:,1:]\n",
    "\n",
    "def save_processed(t_save_path, s_save_path, split=False):\n",
    "    \"\"\"\n",
    "    t Order: Xtrain, ytrain, Xdev, ydeb, Xtest, ytest\n",
    "    s Order: X_train, y_train, X_unl, y_unl\n",
    "    \"\"\"\n",
    "    with open(t_save_path,'wb') as f: pickle.dump([Xtrain, ytrain, Xdev, ydev, Xtest, ytest], f)\n",
    "    if split:\n",
    "        with open(s_save_path,'wb') as f: pickle.dump([X_train, y_train, X_unl, y_unl], f)\n",
    "\n",
    "def load_processed(t_save_path, s_save_path, split=False):\n",
    "    \"\"\"\n",
    "    t Order: Xtrain, ytrain, Xdev, ydeb, Xtest, ytest\n",
    "    s Order: X_train, y_train, X_unl, y_unl\n",
    "    \"\"\"\n",
    "    with open(t_save_path,'rb') as f: t = pickle.load(f)\n",
    "    if split:\n",
    "        with open(s_save_path,'rb') as f: s = pickle.load(f)\n",
    "        return t, s\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-8\n",
    "LATENT_Z = 100\n",
    "max_seq_len = 250\n",
    "path_bert_model = './BERT-models/BERT-Mini'\n",
    "# path_bert_model = './BERT-models/BERT-Mini'\n",
    "\n",
    "path_train = 'dataset/Raw/3c_Train.csv'\n",
    "path_test = 'dataset/Raw/3c_Test.csv'\n",
    "\n",
    "dataset = data.Dataset()\n",
    "dataset.load_csv(path_train, path_test, label_name='blabel', separator='\\t')\n",
    "dataset.make_dev_split(dev_split=0.15)\n",
    "fig = dataset.classes_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, s = load_processed(\"dataset/splitedData/3c_BERT_processed_Data/5percent/Total\", \\\n",
    "               \"dataset/splitedData/3c_BERT_processed_Data/5percent/Split\", split=True)\n",
    "Xtrain, ytrain, Xdev, ydev, Xtest, ytest = t\n",
    "X_train, y_train, X_unl, y_unl = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (Only if processed data is not available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_train().review = dataset.get_train().review.replace('\"','', regex=True)\n",
    "dataset.get_dev().review = dataset.get_dev().review.replace('\"','', regex=True)\n",
    "dataset.get_test().review = dataset.get_test().review.replace('\"','', regex=True)\n",
    "\n",
    "tokenizer = createTokenizer(path_bert_model)\n",
    "\n",
    "Xtrain = np.asarray([format_text(text, tokenizer) for text in dataset.get_train().review])\n",
    "Xtest = np.asarray([format_text(text, tokenizer) for text in dataset.get_test().review])\n",
    "Xdev = np.asarray([format_text(text, tokenizer) for text in dataset.get_dev().review])\n",
    "\n",
    "ytrain = to_categorical(dataset.get_train_y()-dataset.get_train_y().min())\n",
    "ytest = to_categorical(dataset.get_test_y()-dataset.get_test_y().min())\n",
    "ydev = to_categorical(dataset.get_dev_y()-dataset.get_dev_y().min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unl_ratio = 0.99 # 1 - unl_ratio = % train\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=unl_ratio, random_state=0)\n",
    "for train_index, test_index in sss.split(Xtrain, ytrain):\n",
    "    X_train, X_unl = Xtrain[train_index], Xtrain[test_index]\n",
    "    y_train, y_unl = ytrain[train_index], ytrain[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_processed(\"../GAN_BERT/dataset/splitedData/3c_BERT_processed_Data/1percent/Total\", \"../GAN_BERT/dataset/splitedData/3c_BERT_processed_Data/1percent/Split\", split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, s = load_processed(\"dataset/splitedData/3c_BERT_processed_Data/1percent/Total\", \\\n",
    "               \"dataset/splitedData/3c_BERT_processed_Data/1percent/Split\", split=True)\n",
    "Xtrain, ytrain, Xdev, ydev, Xtest, ytest = t\n",
    "X_train, y_train, X_unl, y_unl = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1(tfa.metrics.F1Score):\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = metric_mask_filter(y_true, y_pred)\n",
    "        if self.threshold is None:\n",
    "            threshold = tf.reduce_max(y_pred, axis=-1, keepdims=True)\n",
    "            y_pred = tf.logical_and(y_pred >= threshold, tf.abs(y_pred) > 1e-12)\n",
    "        else:\n",
    "            y_pred = y_pred > self.threshold\n",
    "\n",
    "        y_true = tf.cast(y_true, self.dtype)\n",
    "        y_pred = tf.cast(y_pred, self.dtype)\n",
    "\n",
    "        def _weighted_sum(val, sample_weight):\n",
    "            if sample_weight is not None:\n",
    "                val = tf.math.multiply(val, tf.expand_dims(sample_weight, 1))\n",
    "            return tf.reduce_sum(val, axis=self.axis)\n",
    "\n",
    "        self.true_positives.assign_add(_weighted_sum(y_pred * y_true, sample_weight))\n",
    "        self.false_positives.assign_add(\n",
    "            _weighted_sum(y_pred * (1 - y_true), sample_weight)\n",
    "        )\n",
    "        self.false_negatives.assign_add(\n",
    "            _weighted_sum((1 - y_pred) * y_true, sample_weight)\n",
    "        )\n",
    "        self.weights_intermediate.assign_add(_weighted_sum(y_true, sample_weight))\n",
    "        \n",
    "        \n",
    "def model_classification_loss(y_true, y_pred):\n",
    "    #y_true, y_pred = metric_mask_filter(y_true, y_pred)\n",
    "    #log_probs = tf.nn.log_softmax(y_pred, axis=-1)\n",
    "    #per_example_loss = -tf.reduce_sum(y_true * log_probs, axis=-1)\n",
    "    #loss = tf.reduce_mean(per_example_loss)\n",
    "    \n",
    "    #return tf.cond(tf.equal(tf.size(y_true), 0), lambda: tf.convert_to_tensor(0.), lambda: tf.reduce_mean(per_example_loss))\n",
    "    \n",
    "    y_true, y_pred = metric_mask_filter(y_true, y_pred)\n",
    "    probs = tf.clip_by_value(tf.nn.softmax(y_pred, axis=-1), epsilon, 1. - epsilon)\n",
    "    per_example_loss = -tf.reduce_sum(y_true * tf.math.log(probs), axis=-1)\n",
    "    \n",
    "    return tf.reduce_mean(per_example_loss)\n",
    "\n",
    "def myloss2(y_true, y_pred):\n",
    "\n",
    "    return tf.reduce_mean(tf.math.square(tf.reduce_mean(model.d(model.b(model._X_train_batch))[1], axis=0) - \n",
    "                                   tf.reduce_mean(model.d(model.g(model._noise))[1], axis=0)))\n",
    "\n",
    "def myloss1(y_true, y_pred):\n",
    "\n",
    "    fake_prob = tf.nn.softmax(y_pred, axis=-1)\n",
    "    return -1 * tf.reduce_mean(tf.math.log(tf.clip_by_value((1 - fake_prob[:, 0]), epsilon, 1. - epsilon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_optimizer(tf.keras.optimizers.Adam):\n",
    "    def _clip_gradients(self, grads):\n",
    "        \"\"\"Clip gradients according to the clipnorm and clipvalue attributes.\"\"\"\n",
    "        if self.clipnorm is not None:\n",
    "            if distribute_ctx.has_strategy():\n",
    "                raise ValueError(\"Gradient clipping in the optimizer \"\n",
    "                                 \"(by setting clipnorm or clipvalue) is currently \"\n",
    "                                 \"unsupported when using a distribution strategy.\")\n",
    "            #grads = [None if g is None else clip_ops.clip_by_norm(g, self.clipnorm) for g in grads]\n",
    "            grads, _ = clip_ops.clip_by_global_norm(grads, self.clipnorm)\n",
    "        if self.clipvalue is not None:\n",
    "            if distribute_ctx.has_strategy():\n",
    "                raise ValueError(\"Gradient clipping in the optimizer \"\n",
    "                                 \"(by setting clipnorm or clipvalue) is currently \"\n",
    "                                 \"unsupported when using a distribution strategy.\")\n",
    "            v = self.clipvalue\n",
    "            grads = [None if g is None else clip_ops.clip_by_value(g, -v, v) for g in grads]\n",
    "        return grads\n",
    "    \n",
    "class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Applys a warmup schedule on a given learning rate decay schedule.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            initial_learning_rate,\n",
    "            decay_schedule_fn,\n",
    "            warmup_steps,\n",
    "            power=1.0,\n",
    "            name=None):\n",
    "        super(WarmUp, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.power = power\n",
    "        self.decay_schedule_fn = decay_schedule_fn\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or 'WarmUp') as name:\n",
    "            # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n",
    "            # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "            global_step_float = tf.cast(step, tf.float32)\n",
    "            warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "            warmup_percent_done = global_step_float / warmup_steps_float\n",
    "            warmup_learning_rate = (\n",
    "                self.initial_learning_rate *\n",
    "                tf.math.pow(warmup_percent_done, self.power))\n",
    "            return tf.cond(global_step_float < warmup_steps_float,\n",
    "                           lambda: warmup_learning_rate,\n",
    "                           lambda: self.decay_schedule_fn(step),\n",
    "                           name=name)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'initial_learning_rate': self.initial_learning_rate,\n",
    "            'decay_schedule_fn': self.decay_schedule_fn,\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "            'power': self.power,\n",
    "            'name': self.name\n",
    "        }\n",
    "    \n",
    "def PolynomialDecay_learning_rate(init_lr, num_train_steps, num_warmup_steps):\n",
    "    \n",
    "    learning_rate = tf.optimizers.schedules.PolynomialDecay(\n",
    "        init_lr,\n",
    "        num_train_steps)\n",
    "    if num_warmup_steps:\n",
    "        learning_rate = WarmUp(initial_learning_rate=init_lr,\n",
    "                                  decay_schedule_fn=learning_rate,\n",
    "                                  warmup_steps=num_warmup_steps)\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSGAN_BERT():\n",
    "    def __init__(self, path_bert_model=path_bert_model, split_data=None, adapter_size = 4, adapter_init_scale = 1e-5, \\\n",
    "                 closs_w=1, g_lr=4e-5, d_lr=4e-5, max_seq_len = 250, bilstm=True, latent_dim = 100, g_output_size = 500, \\\n",
    "                 d_hidden_size=128, g_hidden_size=500, d_num_hidden_discriminator = 1, d_drop_out = 0.3, \\\n",
    "                 g_num_hidden_discriminator = 3, g_drop_out = 0.3, num_labels = 4):\n",
    "        \n",
    "        self.path_bert_model = path_bert_model\n",
    "        self.split_data = split_data\n",
    "        self.adapter_size = adapter_size\n",
    "        self.adapter_init_scale = adapter_init_scale\n",
    "        self.closs_w = closs_w\n",
    "        self.g_lr = g_lr\n",
    "        self.d_lr = d_lr\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.bilstm = bilstm\n",
    "        self.latent_dim = latent_dim\n",
    "        self.g_output_size = g_output_size\n",
    "        self.d_hidden_size = d_hidden_size\n",
    "        self.g_hidden_size = g_hidden_size\n",
    "        self.d_num_hidden_discriminator = d_num_hidden_discriminator\n",
    "        self.d_drop_out = d_drop_out\n",
    "        self.g_num_hidden_discriminator = g_num_hidden_discriminator\n",
    "        self.g_drop_out = g_drop_out\n",
    "        self.num_labels = num_labels\n",
    "        self.val_f1 = 0.0\n",
    "        self._noise = None\n",
    "        self._X_train_batch = None\n",
    "        self.create_models()\n",
    "        self.optimizers = None\n",
    "\n",
    "    def create_models(self):\n",
    "        self.b = self.bert_model(self.max_seq_len, self.path_bert_model, self.adapter_size, self.adapter_init_scale)\n",
    "\n",
    "        self.d = self.discriminator(self.b.output.shape[-1], self.d_hidden_size, self.d_num_hidden_discriminator, self.d_drop_out, self.num_labels)\n",
    "        self.D_Model = keras.Model(self.b.input, self.d(self.b.output), name='GAN_Discriminator')\n",
    "\n",
    "        self.g = self.generator(self.latent_dim, self.g_output_size, self.g_hidden_size, self.g_num_hidden_discriminator, self.g_drop_out)\n",
    "        self.G_Model = keras.Model(self.g.input, self.d(self.g.output), name='GAN_Generator')\n",
    "        \n",
    "    def compile_models(self):\n",
    "    \n",
    "        #num_train_steps = int(num_train_examples / batch_size * epochs)\n",
    "        #num_warmup_steps = int(num_train_steps * warm_up_steps)\n",
    "        #D_model_lr, G_model_lr = self.d_lr, self.g_lr\n",
    "        #if decay:\n",
    "            #D_model_lr = PolynomialDecay_learning_rate(self.d_lr, num_train_steps, num_warmup_steps)\n",
    "            #G_model_lr = PolynomialDecay_learning_rate(self.g_lr, num_train_steps, num_warmup_steps)\n",
    "        \n",
    "        if self.optimizers:\n",
    "            D_Model_optimizer = self.optimizers[self.D_Model.name]\n",
    "            G_Model_optimizer = self.optimizers[self.G_Model.name]\n",
    "        else:\n",
    "            D_Model_optimizer = custom_optimizer(learning_rate=self.d_lr, clipnorm=1., beta_1=0.5) #tf.keras.optimizers.Adam(learning_rate=self.d_lr),#'Adam' \n",
    "            G_Model_optimizer = custom_optimizer(learning_rate=self.g_lr, clipnorm=1., beta_1=0.5)\n",
    "            \n",
    "        self.D_Model.compile(loss={'discriminator':self.D_Model_loss()},\n",
    "                             optimizer=D_Model_optimizer,\n",
    "                             metrics={'discriminator': [model_classification_loss, F1(num_classes=(self.num_labels-1), average='macro')]})\n",
    "        \n",
    "        self.d.trainable = False\n",
    "        \n",
    "        self.G_Model.compile(loss={'discriminator':self.G_Model_loss()},\n",
    "                             optimizer=G_Model_optimizer,\n",
    "                             metrics={'discriminator': [myloss1, myloss2]}) #tf.keras.optimizers.Adam(learning_rate=self.g_lr))#'Adam'\n",
    "    \n",
    "    def save_models(self, path):\n",
    "    \n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        for model in (self.D_Model, self.G_Model):\n",
    "            \n",
    "            model.save_weights(path+'/'+model.name+'.h5')\n",
    "            \n",
    "            with open(path+'/optimizer_'+model.name, 'wb') as f:\n",
    "                pickle.dump(model.optimizer, f)\n",
    "        \n",
    "    def load_models(self, path):\n",
    "        \n",
    "        self.optimizers = {}\n",
    "        for model in (self.D_Model, self.G_Model):\n",
    "            model.load_weights(path+'/'+model.name+'.h5')\n",
    "            \n",
    "            with open(path+'/optimizer_'+model.name, 'rb') as f:\n",
    "                self.optimizers[model.name] = pickle.load(f)                \n",
    "        \n",
    "    def G_Model_loss(self):\n",
    "        \n",
    "        def loss (y_true, y_pred):\n",
    "            \n",
    "            fake_prob = tf.nn.softmax(y_pred, axis=-1)\n",
    "            \n",
    "            g_loss_1 = -1 * tf.reduce_mean(tf.math.log(tf.clip_by_value((1 - fake_prob[:, 0]), epsilon, 1. - epsilon)))\n",
    "            \n",
    "            g_loss_2 = tf.reduce_mean(tf.math.square(tf.reduce_mean(self.d(self.b(self._X_train_batch))[1], axis=0) - \n",
    "                                           tf.reduce_mean(self.d(self.g(self._noise))[1], axis=0)))            \n",
    "            return g_loss_1 + g_loss_2\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def D_Model_loss(self):\n",
    "            \n",
    "        def loss(y_true, y_pred):\n",
    "            \n",
    "            prob = tf.clip_by_value(tf.nn.softmax(y_pred, axis=-1), epsilon, 1. - epsilon)\n",
    "            \n",
    "            ### SUPERVISED ###\n",
    "            \n",
    "            logits_y_pred = y_pred[:,1:]\n",
    "            \n",
    "            log_probs= tf.clip_by_value(tf.nn.softmax(logits_y_pred, axis=-1), epsilon, 1. - epsilon)\n",
    "            log_probs = tf.math.log(log_probs)\n",
    "            \n",
    "            one_hot_labels = y_true[:,1:]\n",
    "            label_mask = get_label_mask(y_true)\n",
    "            \n",
    "            per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "            per_example_loss = tf.boolean_mask(per_example_loss, label_mask)\n",
    "            \n",
    "            labeled_example_count = tf.cast(tf.size(per_example_loss), tf.float32)\n",
    "            D_L_Supervised = tf.divide(tf.reduce_sum(per_example_loss), tf.maximum(labeled_example_count, 1)) #* self.closs_w\n",
    "            \n",
    "            ### UNSUPERVISED ###\n",
    "            \n",
    "            #### REAL EXAMPLES ####\n",
    "            \n",
    "            D_L_unsupervised1U = -1 * tf.reduce_mean(tf.math.log(tf.clip_by_value((1 - prob[:, 0]), epsilon, 1. - epsilon)))\n",
    "            \n",
    "            #### FAKE EXAMPLES ####\n",
    "            \n",
    "            #X_noise = tf.random.uniform([tf.shape(y_pred)[0], self.latent_dim], minval=0, maxval=1, dtype=tf.float32)\n",
    "            logits_fake = self.d(self.g(self._noise))[0]\n",
    "            fake_prob = tf.nn.softmax(logits_fake, axis=-1)\n",
    "            fake_prob = tf.clip_by_value(fake_prob, epsilon, 1. - epsilon)\n",
    "\n",
    "            D_L_unsupervised2U = -1 * tf.reduce_mean(tf.math.log(fake_prob[:, 0]))\n",
    "            \n",
    "            return  D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def discriminator(self, d_input_size, d_hidden_size, num_hidden_discriminator, dropout, num_labels):\n",
    "        shared_model_input = keras.layers.Input(shape=(d_input_size,), dtype='float32', name='d_l_i')\n",
    "        hidden_layer = keras.layers.Dropout(dropout, name='d_dp_i')(shared_model_input)\n",
    "        for i in range(num_hidden_discriminator):\n",
    "            hidden_layer = keras.layers.Dense(d_hidden_size, activation=tf.keras.layers.LeakyReLU(), name='d_l_'+str(i))(hidden_layer)\n",
    "            hidden_layer = keras.layers.Dropout(dropout, name='d_dp_'+str(i))(hidden_layer)\n",
    "        \n",
    "        #last_layer = keras.layers.Dense(32, activation=tf.keras.layers.LeakyReLU(), name='d128_last')(hidden_layer)\n",
    "        last_layer = keras.layers.Dense(num_labels, name='d_last')(hidden_layer)\n",
    "\n",
    "        return keras.Model(shared_model_input, [last_layer, hidden_layer], name='discriminator')\n",
    "\n",
    "    def generator(self, latent_dim, g_output_size, g_hidden_size, num_hidden_discriminator, dropout):\n",
    "        shared_model_input = keras.layers.Input(shape=(latent_dim,), dtype='float32', name='g_l_i')\n",
    "        hidden_layer = shared_model_input\n",
    "        for i in range(num_hidden_discriminator):\n",
    "            hidden_layer = keras.layers.Dense(g_hidden_size, activation=tf.keras.layers.LeakyReLU(), name='g_l_'+str(i))(hidden_layer)#keras.layers.ReLU()\n",
    "            hidden_layer = keras.layers.Dropout(dropout, name='g_dp_'+str(i))(hidden_layer)\n",
    "        last_hidden_layer = keras.layers.Dense(g_output_size, activation='tanh', name='g_l_f')(hidden_layer)#keras.layers.LeakyReLU() #'tanh', activation='tanh'\n",
    "        return keras.Model(shared_model_input, last_hidden_layer, name='generator')\n",
    "\n",
    "    def bert_model(self, max_seq_len, path_bert_model, adapter_size, adapter_init_scale):\n",
    "        l_input_ids = tf.keras.layers.Input(shape=(max_seq_len,), dtype='int32', name='input_layer')\n",
    "\n",
    "        bert_params = bert.params_from_pretrained_ckpt(path_bert_model)\n",
    "        bert_params.adapter_size = adapter_size\n",
    "        bert_params.adapter_init_scale = adapter_init_scale\n",
    "        l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "        embedded_sequences = l_bert(l_input_ids) # output: [batch_size, max_seq_len, hidden_size]\n",
    "        \n",
    "        if self.bilstm:\n",
    "            drop_out_1 = tf.keras.layers.Dropout(self.d_drop_out, name='drop_out_1')(embedded_sequences)\n",
    "            cls_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(250))(drop_out_1)\n",
    "            #biLSTM_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(250))(drop_out_1)\n",
    "            #cls_out = tf.keras.layers.Dropout(self.d_drop_out, name='drop_out_2')(biLSTM_1)\n",
    "        else:\n",
    "            cls_out = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :], name='bert_cls')(embedded_sequences)\n",
    "\n",
    "        model = tf.keras.Model(inputs=l_input_ids, outputs=cls_out, name='BERT')\n",
    "        model.build(input_shape=(None,max_seq_len))\n",
    "\n",
    "        l_bert.apply_adapter_freeze()\n",
    "        bert_ckpt_file = os.path.join(path_bert_model, \"bert_model.ckpt\")\n",
    "        bert.load_stock_weights(l_bert, bert_ckpt_file)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def train_generator_by_rep(self, G_loss, D_loss, valid, rep):\n",
    "        aux = div = 1\n",
    "        aux_g_loss = aux_g_b_loss = aux_g_match_loss = 0\n",
    "        if G_loss > D_loss:\n",
    "                aux = div = rep\n",
    "        while aux:\n",
    "            _, g_loss, g_b_loss, g_match_loss = self.G_Model.train_on_batch(self._noise, valid)\n",
    "            aux_g_loss += g_loss\n",
    "            aux_g_b_loss += g_b_loss\n",
    "            aux_g_match_loss += g_match_loss\n",
    "            aux -= 1\n",
    "        return aux_g_loss/div, aux_g_b_loss/div, aux_g_match_loss/div\n",
    "    \n",
    "    def train_generator_by_lr_update(self, G_loss, D_loss, valid, g_lr_update):\n",
    "        \n",
    "        if G_loss > D_loss:\n",
    "            self.G_Model.optimizer.lr.assign(g_lr_update)\n",
    "        \n",
    "        _, g_loss, g_b_loss, g_match_loss = self.G_Model.train_on_batch(self._noise, valid)\n",
    "        \n",
    "        self.G_Model.optimizer.lr.assign(self.g_lr)\n",
    "        \n",
    "        return g_loss, g_b_loss, g_match_loss\n",
    "    \n",
    "    def train(self, X, y, epochs, batch_size=32, X_dev=None, y_dev=None, unl_ratio=0.95, g_rep=10, g_lr_update=1e-3, checkpoint=None, chck_path=None):\n",
    "        #---------------\n",
    "        # inicializar variables\n",
    "        #---------------\n",
    "        \n",
    "        val_f1 = D_loss = C_loss = D_f1 = G_loss = G_match_loss = G_b_loss = 0.0\n",
    "        \n",
    "        #---------------\n",
    "        # compilando modelos\n",
    "        #---------------\n",
    "        print(\"Compilando modelos...\", end=\"\\r\")\n",
    "        \n",
    "        if checkpoint:\n",
    "            self.load_models(checkpoint)\n",
    "            \n",
    "        self.compile_models()\n",
    "            \n",
    "        #---------------\n",
    "        # datos reales sin etiqueta\n",
    "        #---------------\n",
    "    \n",
    "        print(\"Configurando lotes de datos...\", end=\"\\r\")\n",
    "        \n",
    "        if self.split_data == None:\n",
    "            sss = StratifiedShuffleSplit(n_splits=2, test_size=unl_ratio, random_state=0)\n",
    "            for train_index, test_index in sss.split(X, y):\n",
    "                X_train, X_unl = X[train_index], X[test_index]\n",
    "                y_train, y_unl = y[train_index], y[test_index]\n",
    "        else:\n",
    "            X_train, y_train, X_unl, y_unl = self.split_data\n",
    "            \n",
    "        y_unl = np.zeros((y_unl.shape[0], 4))\n",
    "        y_unl[:,0] = 1\n",
    "        y_train = np.concatenate((np.zeros((y_train.shape[0], 1)), y_train), axis=-1)\n",
    "        \n",
    "        X_ttrain = np.concatenate((X_train, X_unl), axis=0)\n",
    "        y_ttrain = np.concatenate((y_train, y_unl), axis=0)\n",
    "        \n",
    "        #---------------\n",
    "        # epoch para training\n",
    "        #---------------\n",
    "        print(\"Evaluando métricas iniciales...\", end=\"\\r\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            _d_loss = _c_loss = _d_f1 = _g_loss = _g_b_loss = _g_match_loss = 0.0\n",
    "            \n",
    "            #return X_ttrain, y_ttrain, batch_size\n",
    "            X_train, y_train = StratifiedBatches([X_ttrain, y_ttrain], batch_size)\n",
    "            self._noise = np.random.uniform(0, 1, (X_dev.shape[0], self.latent_dim))\n",
    "            steps_4_epoch = len(X_train)\n",
    "            \n",
    "            if X_dev is not None and y_dev is not None:# and epoch>0:\n",
    "                y_dev_ = np.concatenate((np.zeros((y_dev.shape[0], 1)), y_dev), axis=-1)\n",
    "                _, _, val_c_loss, val_f1 = self.D_Model.evaluate(X_dev, y_dev_, verbose=0)\n",
    "                if val_f1 > self.val_f1:\n",
    "                    self.val_f1 = val_f1\n",
    "                    self.save_models(chck_path)\n",
    "                    \n",
    "            for batch in range(0, steps_4_epoch):\n",
    "                \n",
    "                if np.isnan(np.sum(X_train[batch])) or np.isnan(np.sum(y_train[batch])):\n",
    "                    print(\"nan exists in this batch\")\n",
    "                    return X_train[batch], y_train[batch]\n",
    "                \n",
    "                self._noise = np.random.uniform(0, 1, (len(X_train[batch]), self.latent_dim))\n",
    "                self._X_train_batch = X_train[batch]\n",
    "                \n",
    "                # ---------------------\n",
    "                #  Train Discriminator with bert D_MODEL\n",
    "                # ---------------------\n",
    "                \n",
    "                _, d_loss, c_loss, d_f1 = self.D_Model.train_on_batch(np.array(X_train[batch]), np.array(y_train[batch]))\n",
    "                    \n",
    "                # ---------------------\n",
    "                #  Train Generator G_MODEL\n",
    "                # ---------------------\n",
    "\n",
    "                valid = np.zeros((X_train[batch].shape[0], 4))\n",
    "                \n",
    "                _, g_loss, g_b_loss, g_match_loss = self.G_Model.train_on_batch(self._noise, valid)\n",
    "                #g_loss, g_b_loss, g_match_loss = self.train_generator_by_rep(G_loss, D_loss, valid, g_rep)\n",
    "                #g_loss, g_b_loss, g_match_loss = self.train_generator_by_lr_update(G_loss, D_loss, valid, g_lr_update)\n",
    "                \n",
    "                # ---------------------\n",
    "                #  Calc and Plot Metrics\n",
    "                # ---------------------\n",
    "                \n",
    "                _d_loss += d_loss\n",
    "                _c_loss += c_loss\n",
    "                _d_f1 += d_f1\n",
    "                _g_loss += g_loss\n",
    "                _g_b_loss += g_b_loss\n",
    "                _g_match_loss += g_match_loss\n",
    "                \n",
    "                D_loss = _d_loss/(batch+1)\n",
    "                C_loss = _c_loss/(batch+1) \n",
    "                D_f1 = _d_f1/(batch+1) \n",
    "                G_loss = _g_loss/(batch+1)\n",
    "                G_b_loss = _g_b_loss/(batch+1)\n",
    "                G_match_loss = _g_match_loss/(batch+1)\n",
    "                \n",
    "                # Plot the progress\n",
    "                log = \"Epoch:\"+str(epoch+1)+\" Batch:(\"+str(batch+1)+\"/\"+str(steps_4_epoch)+\")\"\n",
    "                log = log+\" [loss: \"+\"{:.5f}\".format(C_loss)+ \", macroF1: \"+\"{:.5f}\".format(D_f1)+\"]\"\n",
    "                log = log+\" [D loss: \"+\"{:.5f}\".format(D_loss)+\", G loss: \"+\"{:.5f}\".format(G_loss)+\"]\"\n",
    "                log = log+\" [G b: \"+\"{:.5f}\".format(G_b_loss)+\", G match: \" + \"{:.5f}\".format(G_match_loss) + \"]\"\n",
    "                if X_dev is not None and y_dev is not None:\n",
    "                    log = log+\" [val_loss: \"+\"{:.5f}\".format(val_c_loss)+ \", val_macroF1: \"+\"{:.5f}\".format(val_f1)+\"]\"\n",
    "                print(log, end=\"\\r\")\n",
    "                    \n",
    "            print(\"\")\n",
    "            \n",
    "        self.save_models(chck_path+\"_F\") \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SSGAN_BERT(path_bert_model=path_bert_model, split_data=s, closs_w=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train(Xtrain, ytrain, X_dev=Xdev, y_dev=ydev, epochs=400, batch_size=200, chck_path='./model/T128_1percent_beta1_05') # macro 0.63053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.D_Model.predict(Xtest)\n",
    "y_test = ytest.argmax(axis=1)\n",
    "result = y_pred[0][:,1:].argmax(axis=1)\n",
    "print(classification_report(y_test, result, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load last saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model = SSGAN_BERT(path_bert_model=path_bert_model, split_data=s, closs_w=1)\n",
    "last_model.load_models('./model/T128_1percent_beta1_05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = last_model.D_Model.predict(Xtest)\n",
    "y_test = ytest.argmax(axis=1)\n",
    "result = y_pred[0][:,1:].argmax(axis=1)\n",
    "print(classification_report(y_test, result, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
